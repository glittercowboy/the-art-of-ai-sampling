Hello and welcome back to another episode of Tash Teaches.

I'm Tash and I anticipate that this following video is probably going to spark some amount of controversy.

Maybe even resulting in some of my long-term subscribers unsubscribing.

And the comment section being peppered with sentiments such as "You're a shill for big tech."

And "Tash I thought you were cool but you're just a scumbag."

And I kind of want to say I'm okay with that.

In the past couple of months I've been making a lot of content that is, to my intention, authentic.

I.e.

I am expressing the views that I hold to be true.

I'm not saying that they're true for everybody.

I'm not saying necessarily that you need to change the way that you think and behave in life in order to align with mine.

But the most important thing for me is that the content that I'm making, whether it's videos, whether it's music, whatever it is that I'm doing...

I want to make sure that everything I am saying, everything I am doing is something that I wholeheartedly believe in.

I also want to add one more caveat which is the fact that I change my mind sometimes.

And it's very rarely because somebody told me to.

It's because of the fact that the experience of life that I've had has led me to change my opinion.

So while I'm not saying that I necessarily think I will change my mind on the things I'm about to say, I very well might.

So if you've ever heard the term "strong views loosely held", I think that pretty much sums up the way that I am going to be trying to engage with people on this YouTube channel moving forward.

Anyway, the reason why today is going to be controversial is because I want to open up about my views on using AI in music.

A lot of my favourite YouTubers, people like Ben Jordan and Venus Theory, make amazing videos.

But I feel that there is a predominant hatred or a predominant fear towards the future of AI-collaborated music.

And so I think it's important to add perhaps a slightly more nuanced opinion to the mix because it does seem as though the only kind of YouTube videos that exist are either total for it or total against it.

And I think I find myself somewhere in the middle, perhaps leaning a little bit more towards the for side.

Also, I haven't prepared a speech here, so obviously it's going to be just the same as when I was telling you about moving to Costa Rica and all that shit.

I may jump around all over the place, and for that I request your permission.

I have made a couple of random notes on a piece of paper here, so let me just cross off "anticipate controversy".

Okay, there we go.

So, the first thing I want to say before I dive into all of the AI stuff is I just want to give you a little bit of perspective on not just the way that I make music, but my sentiments around some of the concepts that make up how I like to make music.

I love playing keyboards, and I love playing instruments, and I love, for any of you Bitwig users, making all sorts of cool, random, generated MIDI stuff.

But if I'm being honest, my favourite approach to making music is not working in the MIDI realm.

It is working with audio.

I see audio as just my canvas for chopping, and twisting, and pitching up, and putting backwards, and moving in different orders.

So a lot of the time if I'm working with MIDI stuff anyway, I will export it or bounce it down to audio so that I can treat it like I do a sample.

So, this brings me onto my concept of sampling, and that is that whatever it is that you're working with, the piece of audio, it has with it a certain feeling, right?

And certain things, to me, and I'm not saying to you, but to me, when I'm working with them, they have more or less of amount of feeling.

And I don't just mean feeling as in like, "Oh, that's sad," but there's a texture.

There's almost a memory in the sound, and within that memory exists a story, a narrative, that allows for me, when I'm making music, to really buy in to the creative process, because I'm like, "Oh, this feels like that."

And a lot of the times the things it feels like maybe aren't real, objective truths.

They are subjective experiences I've had in life about memories or experiences I've had as a kid, or perhaps even more fun is my subjective perception of what a thing is.

So, as an example, I've worked with a lot of people who play keys or whatever, and there's like a prompt that I will give them sometimes where I say, "Can you play like it's a smoky bar in like New York at like 2am?"

And that doesn't mean anything, obviously.

It's not an objective thing.

It's not like saying, "Play a rock riff."

You know, it can be interpreted in many different ways.

But I can then say, if they then play something, I'm like, "Yeah," but I can guide them a little bit more with language to try and get to that thing that I can't really put into words anyway, because it's a feeling.

It's a musical feeling that in many ways comes probably from movies I watched as a kid, through books, the way that all these things fuse together to create these subjective understandings of what a feeling is.

So, why I love sampling is because I can be going through YouTube, I can be going through record stores, I can be finding all sorts of obscure source material.

And if something gives me that feeling, that special feeling, oftentimes I want to work with that.

So, what I'll do is I'll record a couple of seconds from a movie or a TV show, or I'll record on my iPhone a band playing next to a river in a crowded city as you walk by, because I personally find that that single piece of recorded audio is more than just some frequencies.

It's more than just maybe a chord progression or a spoken phrase.

It's a meaning.

There is a meaning that I can attribute to that that allows for me, when I'm working with it, to imbue into this music a sense of intention and perhaps even devotion to representing a feeling in sound.

Sampling is dope, particularly if you go and find some sort of 70s disco record, because as I always say to anybody who I'm sure has heard me talk about this, it's like you're not just sampling the music, you're sampling what the bassist had for breakfast.

You're sampling the situation between the studio manager and the secretary on that date.

All of that, all of that information, all of the data that doesn't necessarily exist to the obvious, logical, on-paper conscious mind is all baked into there.

And so I think one of the things that I find most exciting about sampling is that it's approaching a feeling in a way that is perhaps a little bit more non-linear.

Anyway, this is a very long-winded way of just saying the fact that I love working with samples because, A, it means that I can obviously find things that feel a certain way to me and repurpose, repackage, represent them in some way.

But also I'm allowed to then combine these things.

I love the idea of being able to say, "Okay, well, what would it feel like if I took Smokey New York Bar at 2am and combined that with sort of like ritualistic jungle trance-inducing percussion?"

You know, those are just words, but I know personally, for me in my experience, the feeling that those words represent.

And some of the most exciting songs that I've made are where I have fused multiple of these diverse, or what's the word I like to use usually?

I'll probably come back to it, I can't remember right now, but disparate, disparate concepts when you mix them together.

Because for some reason, as a creative, it feels right for me to fuse this with this.

There's a very interesting video by John Cleese, I wonder if I can share it and post it in here, but he talks about the essence of creativity.

And he talks about it in humour, and he says that the most interesting things come from when you fuse things that probably wouldn't necessarily work just on paper.

And I think that's the magic of sampling as well.

So anyways, sampling, very good, love it, I think it's great.

Sure, there are all sorts of ethical concerns and blah blah blah blah blah.

There's no blanket truth when it comes to ethics.

There's no, this is just the answer, I'm going to turn off my screen because there's a slight delay on seeing myself move and it's very irritating.

But yeah, my point is that there is no just blanket truth, therefore, yeah, feel free to disagree with me, feel free to agree with me.

But the most important thing is that you are aware that you have a viewpoint, and your viewpoint in whatever we're talking about here is just as valid as mine, as everybody else's on this planet.

So all I can do is share mine.

So, moving on, let me just cross off my love for sampling.

Okay, I've just put "last three years".

So I've made a couple of videos about this, and as I said at the beginning of this video that I anticipated controversy, these videos tended to be some of my most controversial, the ones that garnered the most unsubscribes, the most comments of disagreement, which is wonderful, I laud that, I applaud it.

And yeah, so over the last three years I've made a couple of videos, but I've never really dived into, like I'm going to today, my excitement about AI when it comes to music.

And I think one of the main issues as well with some of the videos that I made is that perhaps I didn't give enough context into what I mean when I say "using AI in music".

Because there's such a loaded buzzword, it's like a pendulum of thought, that whether it's from other YouTubers, like the ones I've mentioned before, who make amazing content and are absolutely right in their view to have their opinion of it, also the media, you know, whether it's for or against it, we're just pounded with all of these views about, you know, a very loaded topic, a very loaded few words of AI music, like AI-generated music.

And I'm sure if we're going to be talking in just like blanket terms like that, that's going to elicit a lot of very strong emotions from people.

So I want to first just dive into what I mean over what I'm excited about with AI in music.

Because over the last three years I've been exploring a lot of different tools, a lot of very cutting-edge technology that I have to say most people do not know about.

And if they do know about it, they tend to know about the biggest options, or not just the biggest options, but they know about like the most vanilla ways of using them.

Which I am just going to say as a caveat, a garbage.

It's absolute garbage.

So I want to talk first about how I arrived at this.

I got very interested in Mid-Journey when it first came out, because as not just a musical artist, but as a visual artist, and somebody who enjoys the flair of language, should we say, the ability of language to be able to do kind of what music does as well.

It's like if you put the correct words, I say correct, if you put a set of words in an order in a certain way, you can make somebody not just feel a certain way, but open their minds and their hearts to a specific perspective that perhaps only you had somewhere in your psyche.

And so I loved Mid-Journey because it allowed me to fuse two of my favorite things, which was visual art with language.

What do I mean by language?

I'm not talking about putting text on it like graphic design, but I mean as in using my understanding of something, of an art movement in the past, or a specific artist's style.

Being able to describe what I wanted, rather than having to just create it, allowed for me to be able to present myself with options in such a way that I could say yes or no.

Not that no this is bad, or yes this is good, but yes this inspires me, no this doesn't so much.

And then it would allow me to take these things, and with a tool like Photoshop for example, combine them, and use my skills of, you know, 20 years of using Photoshop to be able to create really unique visual experiences.

Sure, for other people, but ultimately and most importantly, for myself.

That's why I make art.

I'm not making art because of the fact that I want to make other people like it and go "Oh, well done Tash, good work."

You know, that feels good, obviously.

I'm not going to say that I don't take quite well to have perhaps people saying that what I've made is good.

You know, there's a little boy in all of us, little boy or girl in all of us that enjoys that creative path on the head.

But most importantly, I enjoy being able to come to reality, to be able to visualize in reality something that before perhaps existed as a hazy thought in my mind.

And so, when I first started using Midjourney, I remember straight away being like "Oh my god, this would be epic if we had this for music.

If we would be able to say 1970s Studio 54 Disco, but with 80s vocal."

And I remember kind of fantasizing about it with my friends, and we all thought "Yeah, that's fantastic."

And so about three years ago maybe, I'm not actually sure on the dates, may have been longer, Suno came around.

I'm sure a bunch of you have heard of Suno.

And what Suno then allowed for was the ability to use, in a pretty basic level, words to create a text-based prompt that could generate sound.

And I'd actually been playing around with a bunch of other things that existed at the same time.

There was Facebook's model, I forget what it was called, but somebody had posted a version of it on Hugging Face.

And I have to admit, it was pretty shit, but at the same time it was my first experience of being able to put text into sound.

And I was instantly hooked.

I was like "This is gonna be, not necessarily the future of music for everyone, but this is gonna be a core part in the future of how I make music."

Because this tech is only gonna get better, only gonna get more responsive, more colorful, more precise.

So when Suno came around, it was a game changer.

Suno in those days was something that you used through Discord, using the "chirp" command I believe it was called.

And it allowed for you to create far more realistic representations of sound.

Now the quality was garbage, it was mono, it was like 128 kilobytes or something, but I saw the potential in this.

Because at the end of the day, I'm a good producer.

There's a reason why I have probably now 71 million streams on the music that I've made without AI.

I've made a lot of music that's connected with people all around the world.

Sure, it's because I can produce, I'm a good producer, but it's mainly because of the emotional resonance, the emotional impact that I've managed to create through the music that ultimately I was making for myself that's connected with others.

So when I was then like, "Okay, well, cool."

So I'm able to now go out and sort of fish for emotional resonance through language being prompted into audio.

And I think a key point that I need to straight away mention here is at no point did I ever think or will I ever think that the benefit of AI-generated music is typing in a prompt, getting a result, and saying that's it.

I think that is akin to just... that's way worse than just going on Splice and finding a construction kit, not even multiple samples, but a construction kit like a defected record, deep house construction kit, and just taking the kick, the top loops, the bass, the chords that are already made and putting them into Ableton and exporting it and saying, "Look what I've made."

I'm not going to hate on that because there are certain people out there who they just don't know any better yet, or maybe one day they will develop a deeper sense of artistic intuition to be able to do something like that.

But I think that the same thing applies with AI music, but it's probably a little bit lamer, which is to just go on there and be like, "Deep house," or like, "Jazz song," and just upload it.

I don't think that's good.

But from those early days, I saw that what I could do with Suno was I was able to generate ingredients, and maybe some of these ingredients, kind of like, let's say, for example, I don't know, squash, for example, in the kitchen.

With a squash, you don't use the skin.

You don't use the seeds.

In order to actually make something with a squash that has grown, that you've picked, "I want that squash," you have to shave off the skin.

You have to scoop out the seeds.

I saw these generations like that, but it's not that I was looking to use all of it.

And I don't mean all of it as in, "This is the length.

I'm going to use this snippet."

I mean, I was very keen on using tools like RipX, which I'd been using for a while before then, to be able to split these stems, to be able to take just isolated parts of it, because those parts still represented the whole of the way that the thing made me feel.

So I started generating things with Suno, and approximating the feelings I was looking to create through language, then splitting the stems and sampling them in exactly the same way I would finding something on Splice, or remove Splice from the mix, going around with my phone, like I often do, and just recording audio, and using that as something to then sample to make a song with, and then play an instrument over the track, and collaborate with other people, and fucking add other stuff.

It's not a case of you just put that generation in and call it a day.

So, another thing also I should probably mention as well is that the audio quality being so bad meant that there was very little that you could do if you weren't then combining these generations with further production.

And that further production part is a key part of the puzzle, which is that you have to be a pretty good producer.

You have to know how to A) make things sound good, B) find things that match well, or coherent blending, curation of sounds, and ultimately you have to know how to finish songs, which most people on earth don't.

You have to be able to actually make an arrangement, not just make a little loop, but have a song that starts from somewhere and goes to somewhere else, and isn't boring, and tells a story, and creates an emotional experience in the listener.

You need all of that stuff to actually make good music with AI in the way that I'm talking about.

Anyways, over those years, Suno got a little bit better.

They brought out their second version that had stereo audio.

Around, you know, at some point in this period of time, it's all a bit of a blur, Udio came out.

And as I understand it, Udio was started by some of the Google AI team, I don't know, whatever.

But Udio presented a far deeper set of controls, because Suno kind of felt a little bit "eh".

And actually, as the models improved on Suno, the overall quality seems to get much worse too.

And I don't mean quality as in audio fidelity, or the fact that it was now stereo, but I mean as in the overall quality of the music.

Everything started to sound, at least to my ears, like shitty library music that YouTubers who make kind of cooking videos put.

You know, like that kind of ukulele sound with like a xylophone.

It was like that kind of garbage.

And so I started to lose a bit of faith in that, until I discovered Udio.

Because Udio had the same thing that Suno had, which is that you write a prompt, and then they have an AI that reinterprets your prompt to give you something that maybe you'll like, or whatever.

But Udio added the ability for manual control.

And I think this is where everything really kicked off for me, and I started to get really, really excited.

It's because of the fact that it allowed me now granular control to be able to get deeper under the hood, and to be able to tweak what I was looking for.

And to be able to use, say, a specific seed value, and the different sliders that come up when you click "Advanced Controls" and put it on "Manual".

To be able to then actually tailor what you were looking for a lot better.

Let's just check that I haven't skipped over anything here.

But yeah, this was now fantastic.

And one of the other things was the quality of music on Udio seemed a lot better.

It actually seemed like, dare I say it, to the untrained ear, or also to some people who I showed, I was like "What do you think of this song?"

And some people were like "Wow, that's kind of cool."

And literally all I did was generate a thing and play it to them on Udio.

So I think it started to get a little bit more indistinguishable.

I'm not saying it is indistinguishable, I still think that human art will 100 billion times always beat AI art.

Because of the magic of intention that AI does not have.

But we'll get to that.

But the point is that the actual sounds that I was then able to generate, and to then split, and to then use as samples in music that I was making, were of a far higher quality.

And again, I don't mean just fidelity, but I mean as in quality.

The innate quality of something where you say "Ah, that is quality."

It had more of that.

So, I'm just going to now skip ahead because I think one of the core elements of what I'm very excited about with AI, whether it is visual or whether you're using LLMs for text stuff like people use ChatGPT for, is that a lot of people just turn up at the party and they just hang out in the first room.

They hang out in the boot room of the party and they're like "Ah, no, it wasn't a great event."

And that's because of the fact that they don't take time to understand that AI can only do as good of a job as you give it instructions.

And so people are like "Write me an email about whatever."

Of course it's going to suck, or of course it's going to be "Eh, vanilla."

And that's because of the fact that you haven't dictated to it what you want.

And so this is a very, very, very interesting part of the whole experience of AI generated anything.

Is that I think that the majority of people on Earth will get bad results with AI for one key reason.

And this extends more than just the things that they're generating with AI.

This extends to their entire lives.

And that is that they don't know what they want in any capacity.

A lot of people know what they don't want, but they don't have a clear understanding of what it is that they do want.

And whether it's the fact that they have never asked themselves the question "What do I want?"

And whether that's because of the fact that they don't believe whatever they would want they would be worthy of or be able to get.

But there's no clarity in their understanding of what it is that they want.

Out of life, out of their experiences on a day-to-day basis, or in this case, out of AI generated music.

So if you don't know how to describe very clearly what it is that you want, you won't be able to get it.

And so I've put here that it's the power of language.

And that is the fact that one of the things that AI has taught me about myself and my art is that I need to be very clear.

I need to be very clear about the results that I'm looking for.

And in order to be clear, I have to be able to explain, not just to the AI, but to myself, what it is that I want.

So when it comes to music, if your avenue for arriving at interesting ingredients that you're generating with AI is language, then you need to be better at language.

You need to understand how to use language to point at intangible, like vague concepts.

And you need to understand how to translate your emotional desires into things that an AI can take as an instruction to give you options for.

And even then, you can be very good at language and AI won't give you the results you necessarily want, because maybe your language on that particular prompt still isn't quite nailing it.

So it's an ever-changing process and the fact that I've had to get really clear about, "Okay, before AI, I've made a lot of music.

I've put out 117 songs now."

And of those 117, only one of them really used AI as the fundamental piece of every sample.

It wasn't just a generation.

It was like 150 generations and I used snippets from each.

Only one of them was something I used entirely AI-generated sounds, except for a shaker.

I recorded the shaker.

But what I mean by this is if I want to continue making music that feels like me using AI as a tool, as a paintbrush, so to speak, or maybe even a canvas, I don't know, I needed to understand what I was already doing.

I needed to understand beyond just, "Oh, Cash Makes Organic House."

I needed to understand what it was that was specific about the music that I made, that allowed me to have the career and the worldwide notoriety for the music that I had made without thinking about what I was doing.

I didn't have to put words to my creations to make them.

But if I were to use AI, I would have to get very clear on not just what my music sounded like, but what the intended feelings were.

What were the emotions that not just were present in the music, but were intended when I was making that music?

And so I'm actually really, really excited when I sit down and make music without AI because I have this very clear roadmap, this guidebook in my mind, and frankly, in a bunch of notes documents now as well, about words that I can use to shape and guide my understanding of what it is that I want to see or hear exist in the world.

And funnily enough, going back to people not knowing what they want, but a lot of the time knowing what they don't want, that is actually a core part of the process.

And so if you just fill in the blank and just learn what you do want, the what you don't want is very crucial too, because audio has the ability to exclude genres or textures or emotions or concepts.

So knowing what you want is as important in my notes sections here as knowing what I don't want.

So I have nothing against people who listen to metal, but I know that I don't want that.

I have no interest.

It doesn't move me emotionally.

I know that specific adjectives that you could use to describe music that don't resonate with me are harsh or aggressive.

Some people love that.

Some people love dubstep and stuff and they're like, "Oh man, this is so aggressive.

Yeah, I'm feeling good."

I don't want that.

So I now have like, thousands of words that I can use to describe music that if it fit with that, dusty, vintage, warm, mellow, melancholic, spiritual.

All of these words, like I could use to describe things I like, whereas like broken, poppy, all of these kinds of things I don't want, you know, or aggressive or yeah, I don't want any of that in my music.

So it's really helped me get clear about what it is that I'm generating.

One of the things that's actually also been very helpful for getting good at prompting audio is ChatGPT and Claude and other various impressive LLMs for sort of text-to-text based stuff.

And I've actually created a bunch of templates, or should I say custom GPTs, that I know that I can rely on to give me variations of things I'm interested in.

And I think this is an interesting point that we should just dive off of as well, that just because I know what I like, doesn't necessarily mean that I have to pretend like I know what I want to hear every time in the moment.

I like to be surprised.

I think, what's his name, Joseph Collier, Jacob Collier, he has a video where he's talking about, he was doing like a live stream and he was showing off some funny AI tool online.

And he was talking about how the fact that he enjoys the element of surprise.

And I think this is something that really hit home for me, is that a lot of the time I know what I want, but I don't necessarily want it exactly like I want it.

And maybe this sounds like a kind of, what's the word, a contradiction to everything I've said before, but I am a walking contradiction, I'm human after all.

But I like to be surprised.

I like to guide something in a direction to say, that's not exactly what I was thinking, but it's almost better than what I was thinking.

And so a lot of these AI things, like my templates that I've set up and these custom GPTs, they know, they know, they have a bunch of data in their knowledge base.

I'm not like their real human collaborator, but they know based off of the thousand plus words in their system and the very specific instructions I've given them on how to present the responses when I ask them for prompts to give me combinations of things.

So for example I can say like, you know, give me a kind of African style prompt, and it will use like Afrobeat, it will use West African percussion, it will say Mali Blues, it will pick random combinations and it will give me these very interesting prompts.

Where at the end of the day I don't have to just go, okay, we'll use that, but it will give me things that I can say, ah, yeah, that's something we should explore, I'm interested in that.

So I'll copy and paste some of the words from one prompt, some from the other, and then I'll chuck it into Udia and I'll get a result.

Maybe I don't quite like what it came out with because maybe the word jazz was in there and I'm not in the mood for jazz.

I'll remove that and I'll try again, maybe even using the same seed value or trying to change the prompt strength.

But anyways, the ability for me to be able to take less time having to consciously think about things and more intuitively, gut based, choose things, decide on things is revolutionary with AI.

Because again, at the end of the day, what matters to me is making music that moves me.

What matters to me is creating art of all kinds that gives me an emotional experience that I go, oh, yeah, that.

And AI has been really, really helpful with that.

I'm actually getting to the end of all of my topics that I actually have to say here, but one of them also, again, is limitations.

And so I think maybe I was talking about this when I was still referencing Suno.

Actually, side note, Suno is still garbage.

Suno has got worse and worse.

They have their current V4 beta.

And I went on it the other day to see, you know, maybe I was like missing out on some amazing new features.

It's shite.

The quality of the stuff that you get on there just feels like Sesame Street garbage.

Udia is still the king.

I cannot wait for the ability to run local instances of these kind of models.

And ideally at a certain point to feed it my own music, to be able to continuously work with getting present, like, representations of the things that I like to create.

But anyway, side note, the limitation factor.

This is still really key because I don't think, and maybe I said this in a past video, but it's like I don't think in the beginning I understood the privilege that I have as a very good producer.

Which is that, like, at this point I don't have to think about what I do.

You know, often times I'll be doing a live stream or I'm teaching a class and I'm talking about what I'm doing in the moment.

And I'm very fast, you know, sometimes I have to slow down.

But I'm ten times, twenty times faster than that when I'm not having to explain what I'm doing.

Everything is, like, before I even think it, I'm doing it.

And that's not necessarily to say every decision I make is correct and I stick with it.

Sometimes I'll delete things.

Often times I delete things.

But what I mean is that I have this very quick, instantaneous, intuitive flow to my music production.

I'm aware that now the sun is probably going to be fucking up the grading of my video because we started darker.

Anyways, the privilege that I have is the fact that I can get these generations, I can split them.

And then obviously there is a bunch of very fucked up spectral smearing issues.

So many artifacts.

We're not at a point yet with AI generated music where it is good quality in the same sense as if you just found something on the splice.

Or you recorded something miles off of that.

The quality is quite bad.

Particularly if you then split something, you know, because the vocal ends up taking a little bit of the high frequencies from the piano that you tried to split.

That's not the end of the world, though, for someone like myself who understands how to make things sound good.

And ultimately how to transform less than ideal content, less than ideal samples or sound sources into something that I can use.

So, one of the most important things I think for producers who are looking to work with AI and not work, have AI generate all their music, obviously context here.

To use AI as an addition to their creativity, an extension of their creative tools.

Is that you have to be really good at producing.

You can't take away from the fact that it's very, very important to know the fundamental production techniques of equalization and compression and spectral repair.

And all of this kind of stuff.

So, yeah.

I probably don't really have too many more points to say on this.

Okay, no, I do actually.

There are three main parts to, I think, this new journey, this undertaking of if you would like to figure out how you can use AI as a tool set.

And those three main parts, I'm just going to sort of summarize everything I've talked about before.

Number one, and the most important thing, is to get super clear on what you're doing and what you want to do.

What message you want to spread to the world, the emotions that you want to capture and represent in sound.

The textures, the times, the eras, the 50s, the 60s, the 90s, the early 1800s, whatever.

The countries, you know, Iran.

Do you like Persian music?

Do you like Turkish music?

Do you like kind of music from the Andes?

You need to understand all of the parts that it is that you like.

Because when you get that, when you get very clear on the specifics of what it is that you want to say and how you want to say it and the tools and textures that you're going to use to share that message, AI becomes your best friend.

So, first point is just to get super clear, you know, define your vision.

Number two is to then get really good at being able to use these AI models.

Because it's not good enough to just go on Udio and Suno and just use the automatic mode to just generate some stuff.

Like, okay, maybe you're going to get a couple of good results, but you're not going to have the level of control that you need as a master artist to be able to tell the stories that you want to tell.

So, you've got to get really good at using these models, at being very clear about what parameters to change, the specifics of prompt words and the orders, how you can use specific prompting structure and say the custom lyrics to be able to guide sections within sections.

You have to get very clear on that.

And then the third step, just because I wrote down two on here, I also didn't want to get...

And then the third step is to get really good at figuring out how to fix the fucked up audio quality.

You have to get really good at your production and figuring out how you can take these less than ideal sounds and create something meaningful and magical with them.

And most importantly, how to arrange full songs that you can release, that you can share with the world and ideally move people with that music.

I'm going to wrap this video up here now because I think I've probably waffled on long enough.